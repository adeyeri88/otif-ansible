---
# tasks file for config_wsm_crawler_vm

- name: Find where Nutch configuration configuration is located
  command: "find / -path '*/conf/nutch-site.xml'"
  register: conf_location

- debug:
    msg: "Nutch configuration file has been found under {{conf_location.stdout}}"


# The Ansible xml module seems to have limitation to deal with schema used by Nutch
#  - name: Set Nutch fetcher.threads.per.queue property to 5
#    xml:
#      path: "{{ conf_location.stdout }}"
#      backup: yes
#      xpath: /configuration
#      add_children: 
#        - property: bla
#      pretty_print: yes
#      state: present

- name: Use updated nutch-site.xml file to replace the default one
  template:
    src: nutch-site.xml.j2
    dest: "{{ conf_location.stdout }}"
    backup: yes

- name: Find where WSM Crawler configuration is located
  command: "find / -path '/OpenText/*/etc/otif-crawler.properties'"
  register: conf_location

- debug:
    msg: "WSM Crawler configuration file has been found under {{conf_location.stdout}}"

- name: Update WSM Crawler configuration file {{conf_location.stdout}}
  become: true
  lineinfile:
    dest: "{{ conf_location.stdout }}"
    regexp: "{{ item.regexp }}"
    line: "{{ item.line }}"
  with_items:
    - { regexp: "^crawler.service.zookeeper.ensemble=", line: "crawler.service.zookeeper.ensemble={{ zookeeper_hostname }}:2181" }
    - { regexp: "^crawler.service.host=", line: "crawler.service.host={{ wsm_crawler_hostname }}" }
    - { regexp: "^crawler.kafka.producer.bootstrap.servers=", line: "crawler.kafka.producer.bootstrap.servers={{ wsm_crawler_hostname }}:9092" }

- name: Find where WSM Crawler Admin configuration is located
  command: "find / -path '/OpenText/*/etc/crawleradmin.properties'"
  register: conf_location

- debug:
    msg: "WSM Crawler Admin configuration file has been found under {{conf_location.stdout}}"

- name: Update WSM Crawler Admin configuration file {{conf_location.stdout}}
  become: true
  lineinfile:
    dest: "{{ conf_location.stdout }}"
    regexp: "{{ item.regexp }}"
    line: "{{ item.line }}"
  with_items:
    - { regexp: "^crawleradmin.db.host=", line: "crawleradmin.db.host={{ wsm_crawler_hostname }}" }
    - { regexp: "^crawleradmin.servicediscovery.zookeeper.ensembleurl=", line: "crawleradmin.servicediscovery.zookeeper.ensembleurl={{ zookeeper_hostname }}:2181" }    

- name: Make sure Kafka/Zookeeper, MongoDB, PostgreSQL and wsm crawler services are running
  systemd:
    state: restarted
    daemon_reload: yes
    enabled: yes
    name: "{{ item.name }}"
  with_items:
    - { name: ot-zk-1 }
    - { name: kafka }
    - { name: mongod }
    - { name: postgresql-9.4 }
    - { name: infofusion-web-crawling-1 }
    - { name: otif-wsm-crawler-1 }
    

- name: Wait for Kafka & Zk services on ports 9092/2181. PostgreSQL on 5432. WSM Crawler services on port 10010/11. MongoDB on 27017
  wait_for:
    host: "127.0.0.1"
    port: "{{ item.port }}"
    state: started
  with_items:
#    - { port: 9092 }
    - { port: 2181 }    
    - { port: 27017 }
    - { port: 5432 }
    - { port: 10010 }
    - { port: 10011 }

    
- name: Install kazoo & pymongo dependency - needed for the Ansible znode module used to talk to Zookeeper
  become: true
  easy_install:
    name: "{{ item }}"
    state: latest
  with_items:
    - kazoo
    - pymongo

- name: Remove znode reserved for Discovery Service (wrong place if it's there!)
  znode:
    hosts: 'localhost:2181'
    name: '/ot'
    recursive: yes
    state: absent

#- name: Add znode for Twitter Crawler in Registray Service
#  znode:
#    hosts: "{{ zookeeper_hostname }}:2181"
#    name: "/ot/service-registry/crawler-twitter/3d846b93-f419-4d16-96e3-6fd76e42b01e"
#    value: "{'name':'crawler-twitter','id':'3d846b93-f419-4d16-96e3-6fd76e42b01e','address':'{{ wsm_crawler_hostname }}','port':null,'sslPort':null,'payload':{'@class':'com.opentext.otif.service.$
#    state: present


# - name: Make sure DB is propertly cponfigured in WSM Cralwer local config
#   property crawleradmin.db.host needs to point to localhost (ansible_hostname)

# - name: make sure Zookeeper is set properly in the WSM Cralwer local config  
#  Property crawleradmin.servicediscovery.zookeeper.ensembleurl should point to the ZK on the pipeline, not the crawler host

- name: Set MongoDB authentication for admin user
  mongodb_user:
    database: admin
    name: "admin"
    password: "admin"
    update_password: on_create
    roles: root 
    state: present

- name: Create magellan db in MongoDB and set authentication for users admin & magellan
  mongodb_user:
    login_user: admin
    login_password: admin
    database: magellan
    name: "{{ item.name }}"
    password: "{{ item.pwd }}"
    update_password: on_create
    roles: "{{ item.roles }}"
    state: present
  with_items:
    - { name: "magellan", pwd: "magellan", roles: "dbOwner,userAdmin" }
    - { name: "admin", pwd: "admin", roles: "userAdmin" }


- name: Authenticate to the WSM Crawler Admin REST API
  uri:
    url: "http://{{ wsm_crawler_fqdn }}:10010/crawlers-admin/rest/v1/authentication/login"
    method: POST
    body: "username=admin&password=admin"
    status_code: 200
    headers:
      Content-Type: "application/x-www-form-urlencoded"
  register: login

- name: Create a sample Twitter Access Token (for testing only!)
  uri:
    url: "http://{{ wsm_crawler_fqdn }}:10010/crawlers-admin/rest/v1/crawlertypes/Twitter/configs"
    method: POST
    body: > 
      {"config_id":"test","props":[{"prop_name":"CONSUMER_KEY","prop_value":"jCF3SP0aFiqkDwvw5UO52pzEm"},
      {"prop_name":"CONSUMER_SECRET","prop_value":"9CJYOOBR9mSBL0gC1alejvyvfICXFiFMts66NKIk3vInQLj3FZ"},
      {"prop_name":"ACCESS_TOKEN","prop_value":"12842652-i1qoXufhBX7ZHyrq6xyA32Z3SGVTvRBRFzyNlWSmz"},
      {"prop_name":"ACCESS_SECRET","prop_value":"COlIFpmBWjaHs66HhkwwIUTFCUkiDCoyfyWaOGtUT5oSb"}]}
    status_code: 201, 400
    body_format: json
    timeout: 10
    headers:
      Cookie: "{{login.set_cookie}}"
  register: request_otif

- name: 
  debug:
    var: request_otif

- name: Create a sample Twitter crawling job
  uri: 
    url: "http://{{ wsm_crawler_fqdn }}:10010/crawlers-admin/rest/v1/jobs"
    method: POST
    body: >
      {"schedule_info":{"repeat_count":0,"recurrence":0},
      "search_text":{"criteria":{"type":"twitter_search","words":{"include_all_these":[],"include_any_of_these":[],
      "not_include_these":[],"include_these_hashtags":["#opentext"],"exact_phrase":null},
      "dates":null,"language":null,"surrounding_to":null,"people":null,
      "others":{"positive":false,"negative":false,"question":false,"include_retweets":false,"result_type":"mixed"}}},
      "job_id":1,"name":"Sample Twitter Crawling Job","status":"Unscheduled",
      "description":"This is a sample Twitter crawling job",
      "crawler_subtype":"Twitter Search","job_group":"OpenText","exec_info":{"id":0,"job_id":1}}
    status_code: 201, 400
    body_format: json
    timeout: 10
    headers:
      Cookie: "{{login.set_cookie}}"
  register: request_otif

- name:
  debug:
    var: request_otif

- name: Create a sample Web crawling job
  uri: 
    url: "http://{{ wsm_crawler_fqdn }}:10010/crawlers-admin/rest/v1/jobs"
    method: POST
    body: >
      {"schedule_info":{"repeat_count":0,"recurrence":0},
      "search_text":{"numberOfCycles":5,"batchSize":50,"configuration":{},
      "urls":["http://investors.opentext.com/press-releases?aa7708e5_year[value]=_none"],
      "url_filters":[{"expression":"http://investors.opentext.com/press-releases*","type":"INCLUDE"},
      {"expression":"http://investors.opentext.com/news-releases/news-release-details/*","type":"INCLUDE"}],
      "scrapingRules":[{"targetField":"document.metadata.attributes.creationDate",
      "textExtraction":{"concatenate":false,"xpaths":["//div[contains(@class,'news-date')]/div/text()"]},
      "textTransformation":[{"type":"dateFormatting","parameters":{"format":"MMM d, YYYY"}}]},
      {"targetField":"document.metadata.extensions.crawler.web_content_source",
      "textExtraction":{"concatenate":false,"xpaths":["//*[@property=\"og:site_name\"]/@content"]},
      "textTransformation":[{"type":"regexSubstitution","parameters":{"regex":"(.*)","replacement":"$1"}}]}]},
      "job_id":2,"name":"Sample Web Crawling Job","status":"Unscheduled",
      "description":"This is a sample Web crawling job","crawler_subtype":"Nutch","job_group":"OpenText","exec_info":{"id":0,"job_id":2}}
    status_code: 201, 400
    body_format: json
    timeout: 10
    headers:
      Cookie: "{{login.set_cookie}}"
  register: request_otif

- name: Create a sample Web crawling job
  uri:
    url: "http://{{ wsm_crawler_fqdn }}:10010/crawlers-admin/rest/v1/jobs"
    method: POST
    body: >
      {"schedule_info":{"repeat_count":0,"recurrence":0},
      "search_text":{"numberOfCycles":3,"batchSize":100,"configuration":{},
      "urls":["https://blogs.opentext.com/"],"url_filters":[{"expression":"/category/","type":"EXCLUDE"},{"expression":"/author/","type":"EXCLUDE"},{"expression":"/wp-json/","type":"EXCLUDE"},
      {"expression":"^https://blogs.opentext.com/*","type":"INCLUDE"},{"expression":".","type":"EXCLUDE"}],
      "scrapingRules":[{"targetField":"document.metadata.attributes.creationDate","textExtraction":{"concatenate":false,"xpaths":["//meta[@property=\"article:published_time\"]/@content"]},
      "textTransformation":[{"type":"dateFormatting","parameters":{"format":"yyyy-MM-dd'T'HH:mm:ssXXX"}}]},
      {"targetField":"document.metadata.extensions.crawler.web_content_source","textExtraction":{"concatenate":false,"xpaths":["//meta[@property=\"og:site_name\"]/@content"]},
      "textTransformation":[]}]},"job_id":3,"name":"OpenText Blogs","creation_date":"2018-03-06T16:28:45.290-0800",
      "status":"Running","date_modified":"2018-03-06T16:51:43.798-0800","crawler_subtype":"Nutch",
      "job_group":"OpenText","exec_info":{"id":3,"job_id":3,"last_run":"2018-03-06T16:45:35.800-0800","status":"Completed","message":"Job execution completed successfully"}}
    status_code: 201, 400
    body_format: json
    timeout: 10
    headers:
      Cookie: "{{login.set_cookie}}"
  register: request_otif


- name:
  debug:
    var: request_otif

- name:
  vars:
    msg: |
         Magellan Crawler for Web & Social Media has been configured on {{ansible_host}}.
         Two sample crawling jobs have bee created to test with.
         You can validate deployement by opening http://{{ansible_hostname}}:10010
         Default credentials are admin/admin.
  debug:
    msg: "{{ msg.split('\n') }}"
